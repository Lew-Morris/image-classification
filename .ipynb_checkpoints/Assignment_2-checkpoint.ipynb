{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {
    "id": "4VQ1YbciZ_PK"
   },
   "source": [
    "#     IMAGE CLASSIFICATION USING FASHION-MNIST DATASET "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "qa3geDR1Z_PM"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in /home/lewis/PycharmProjects/venv/lib/python3.10/site-packages (22.3.1)\r\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-10 10:40:55.702027: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-12-10 10:40:55.984614: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-12-10 10:40:55.984633: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2022-12-10 10:40:56.787855: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2022-12-10 10:40:56.787937: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2022-12-10 10:40:56.787946: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "# Try to update pip\n",
    "try:\n",
    "    !pip install --upgrade pip\n",
    "except:\n",
    "    print(\"Failed to update pip\")\n",
    "\n",
    "# Importing packages and dataset into the workspace\n",
    "try:\n",
    "    %matplotlib inline\n",
    "    import matplotlib.pyplot as plt\n",
    "except:\n",
    "    !pip install matplotlib-inline\n",
    "    %matplotlib inline\n",
    "\n",
    "try:\n",
    "    import tensorflow as tf\n",
    "except:\n",
    "    !pip install tensorflow\n",
    "    import tensorflow as tf\n",
    "finally:\n",
    "    from tensorflow import keras\n",
    "    from keras.utils import to_categorical\n",
    "    from keras.models import Sequential\n",
    "    from keras.layers import Dense, Flatten, Conv2D, Dropout, MaxPooling2D, BatchNormalization\n",
    "\n",
    "try:\n",
    "    import numpy as np\n",
    "except:\n",
    "    !pip install numpy\n",
    "    import numpy as np\n",
    "    \n",
    "try:\n",
    "    import sklearn\n",
    "except:\n",
    "    !pip install scikit-learn\n",
    "    import sklearn\n",
    "finally:\n",
    "    from sklearn.decomposition import PCA\n",
    "    from sklearn.metrics import accuracy_score\n",
    "    from sklearn.model_selection import GridSearchCV, ShuffleSplit, train_test_split\n",
    "\n",
    "fashion_mnist = keras.datasets.fashion_mnist\n",
    "(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oBEzOQ3nZ_PP"
   },
   "source": [
    "## Exploratory data analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 753
    },
    "id": "vKKo91sgZ_PQ",
    "outputId": "66eb8791-7f9c-4366-8945-9c34d58847b3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28)\n",
      "(10000, 28, 28)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "can only concatenate str (not \"numpy.uint8\") to str",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 31\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m# We check the distribution of labels\u001b[39;00m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m10\u001b[39m):\n\u001b[0;32m---> 31\u001b[0m     \u001b[38;5;28mprint\u001b[39m (\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mTrain label \u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtrain_labels\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m count :\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mlen\u001b[39m([j \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m train_labels \u001b[38;5;28;01mif\u001b[39;00m j\u001b[38;5;241m==\u001b[39mi])))\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;28mprint\u001b[39m (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTest label \u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m test_labels[i] \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m count :\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mlen\u001b[39m([j \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m test_labels \u001b[38;5;28;01mif\u001b[39;00m j\u001b[38;5;241m==\u001b[39mi])))\n\u001b[1;32m     36\u001b[0m \u001b[38;5;66;03m# Before procesing further, let us plot some items and their corresponding classes.\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: can only concatenate str (not \"numpy.uint8\") to str"
     ]
    }
   ],
   "source": [
    "# Getting the dimension of the training and testing sets for images and labels. The first value represents the sample size, \n",
    "# while the next values will represent dimensions of the array.\n",
    "print(train_images.shape)\n",
    "print(test_images.shape)\n",
    "\n",
    "# We notice that the labels are given without explaining which clothes they are supposed to represent. Therefore, we make a list \n",
    "# that will store the item names\n",
    "\n",
    "label_names = ['T-shirt/top','Trousers','Pullover','Dress','Coat','Sandal','Shirt','Sneaker','Bag','Ankle boot']\n",
    "\n",
    "# Now, let us normalize the array values\n",
    "\n",
    "train_images = train_images/np.float32(255)\n",
    "test_images = test_images/np.float32(255)\n",
    "\n",
    "# For classification purposes in python, it will be much better to convert our 2d array into a 1d array, filling in row by row.\n",
    "# Then we turn our flattened array into a numpy array object. \n",
    "def convert(nparray):\n",
    "    l=[]\n",
    "    dims=nparray.shape\n",
    "    for i in range(dims[0]):\n",
    "        l.append(nparray[i].flatten())\n",
    "    l=np.array(l)\n",
    "    return l\n",
    "train_images_mod = convert(train_images)\n",
    "test_images_mod  = convert(test_images)\n",
    "\n",
    "# We check the distribution of labels\n",
    "\n",
    "for i in range(10):\n",
    "    print ('Train label ' + train_labels[i] + ' count :' + str(len([j for j in train_labels if j==i])))\n",
    "    print ('Test label ' + test_labels[i] + ' count :' + str(len([j for j in test_labels if j==i])))\n",
    "\n",
    "\n",
    "\n",
    "# Before procesing further, let us plot some items and their corresponding classes.\n",
    "\n",
    "plt.figure(figsize=(6,6))\n",
    "for i in range(9):\n",
    "    plt.subplot(3,3,i+1)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.grid(False)\n",
    "    plt.gray()\n",
    "    plt.imshow(train_images[i])\n",
    "    plt.xlabel(label_names[train_labels[i]])\n",
    "plt.savefig('image_examples.jpg')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sZKlG3w2Z_PS"
   },
   "source": [
    "## Dimensionality reduction - Principal Component Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8Iiytb9iZ_PT"
   },
   "source": [
    "As our data is highly dimensional (784 dimensions), we will resort to the feature selection type of dimensionality reduction,\n",
    "called principal component analysis. We will calculate first 40 principal components. Then we will make a scree plot, and find\n",
    "the number of components that we are going to use in our calculations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3hSIroM9Z_PU"
   },
   "source": [
    "From the plot, we see that the 80% of variance is captured between the before the 25th PC, so we will not work with more than \n",
    "that number of components. However, variance is expalined the most by the first eight PCs, so they will be included in the \n",
    "analysis as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uvs4NNiNZ_PW"
   },
   "source": [
    "### Convolutional neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cU3b0hzuZ_PX",
    "outputId": "879d1918-b1fe-4d36-d08b-7ac668c50f36"
   },
   "outputs": [],
   "source": [
    "# We will first reshape our data, in order to produce viable results. We will choose that our data has 1 channel, which signals\n",
    "# that we have a grayscale image. The labels will be turned to categorical.\n",
    "\n",
    "cnn_train       = train_images_mod.reshape(60000,28,28,1)\n",
    "cnn_test        = test_images_mod.reshape(10000,28,28,1)\n",
    "cnn_label_train = to_categorical(train_labels)\n",
    "cnn_label_test  = to_categorical(test_labels)\n",
    "no_of_epochs    = 50\n",
    "batch_size      = 8\n",
    "\n",
    "\n",
    "# We will also introduce a validation set. In our case, it will be equal to 16% percent of the training set (closest to 10.000)\n",
    "# points from the test set\n",
    "\n",
    "cnn_train, cnn_train_val, cnn_label_train, cnn_label_train_val = train_test_split(cnn_train,cnn_label_train, test_size = 0.16, random_state = 2019)\n",
    "\n",
    "# The model we will build is sequential, going layer by layer. It is also the easiest way to build a model in KERAS.\n",
    "\n",
    "cnn_model = Sequential()\n",
    "# First convolution\n",
    "cnn_model.add(Conv2D(64, kernel_size = (3,3), activation = 'relu' , kernel_initializer = 'he_uniform', input_shape = (28,28,1), trainable=True))\n",
    "cnn_model.add(BatchNormalization())\n",
    "\n",
    "cnn_model.add(MaxPooling2D(pool_size=(1,1), trainable=True))\n",
    "cnn_model.add(BatchNormalization())\n",
    "\n",
    "# Second convolution\n",
    "cnn_model.add(Conv2D(128, kernel_size = (3,3), activation = 'relu', trainable=True))\n",
    "cnn_model.add(BatchNormalization())\n",
    "\n",
    "cnn_model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "cnn_model.add(BatchNormalization())\n",
    "\n",
    "# Flatten layer\n",
    "cnn_model.add(Flatten())\n",
    "\n",
    "# Dense layers\n",
    "# cnn_model.add(Dense(32,activation = 'selu', trainable=True))\n",
    "# cnn_model.add(BatchNormalization())\n",
    "\n",
    "cnn_model.add(Dropout(rate=0.2))\n",
    "\n",
    "cnn_model.add(Dense(128,activation = 'relu', trainable=True))\n",
    "cnn_model.add(BatchNormalization())\n",
    "\n",
    "cnn_model.add(Dense(10,activation='softmax'))\n",
    "\n",
    "\n",
    "lr_schedule = keras.optimizers.schedules.ExponentialDecay(\n",
    "    initial_learning_rate=1e-2,\n",
    "    decay_steps=512,\n",
    "    decay_rate=0.9)\n",
    "\n",
    "\n",
    "# Compile the model\n",
    "cnn_model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "                  optimizer=keras.optimizers.SGD(learning_rate=lr_schedule), # or adam\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "# Here, we are going to fit the model to our dataset.\n",
    "cnn_model_training = cnn_model.fit(cnn_train,\n",
    "                                   cnn_label_train,\n",
    "                                   batch_size=batch_size,\n",
    "                                   epochs=no_of_epochs,\n",
    "                                   verbose=1,\n",
    "                           validation_data=(cnn_train_val,cnn_label_train_val))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IJQwlvPbZ_PY"
   },
   "source": [
    "#### Plotting accuracies and losses of the CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 545
    },
    "id": "kYDGdpS_Z_PY",
    "outputId": "77532edc-dee4-4252-9691-1d2841779a4e"
   },
   "outputs": [],
   "source": [
    "cnn_accuracy     = cnn_model_training.history['accuracy']\n",
    "cnn_val_accuracy = cnn_model_training.history['val_accuracy']\n",
    "loss             = cnn_model_training.history['loss'] \n",
    "val_loss         = cnn_model_training.history['val_loss']\n",
    "epochs = [i for i in range(1, no_of_epochs + 1)]\n",
    "\n",
    "plt.plot(epochs, cnn_accuracy, 'b--', label = 'Test accuracy')\n",
    "plt.plot(epochs, cnn_val_accuracy, 'r', label='Validation accuracy')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.savefig('Accuracy values')\n",
    "plt.legend()\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(epochs, loss, 'b--', label = 'Training loss')\n",
    "plt.plot(epochs, val_loss, 'r', label = 'Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.legend()\n",
    "plt.savefig('Loss values')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CXGRYu0tZ_PZ"
   },
   "source": [
    "##### Remarks about the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-Ty56LrRZ_PZ"
   },
   "source": [
    "From the figures above, we see that the model is performing well. We see that the dataset becomes learned around the 15th epoch, because the validation accuracy from the on hovers around 0.90.  We will now evaluate the model on the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JHviZyWtZ_Pa",
    "outputId": "25bba095-5f83-4eb2-bc73-2bfd4f1c08cb"
   },
   "outputs": [],
   "source": [
    "accuracy = cnn_model.evaluate(cnn_test,cnn_label_test,verbose=1)\n",
    "print('The accuracy of CNN is equal to : ' + str(accuracy[1]))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
